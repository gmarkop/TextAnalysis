"""
Greek Text Analysis Tool - Optimized for GitHub Codespaces

Optimizations for resource-constrained environments:
- Reduced file size limits (10MB for Codespaces vs 50MB)
- Reduced processing limits (500K chars vs 1M chars)
- Intelligent sentence sampling for sentiment/emotion analysis (max 50 sentences)
- Progress indicators for long-running operations
- Reduced ML model iterations (LDA: 10 vs 20, NMF: 100 vs 200)
- Aggressive memory cleanup with garbage collection
- Memory usage monitoring and warnings
- Lazy loading of heavy transformer models
"""

import streamlit as st
import nltk
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.probability import FreqDist
from collections import Counter
import re
import numpy as np
import spacy
import matplotlib
import warnings
from functools import lru_cache
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation, NMF
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, XLMRobertaTokenizer
import torch
import psutil
import os
import gc

# Set environment variables to prevent threading issues
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["OMP_NUM_THREADS"] = "1"
os.environ["MKL_NUM_THREADS"] = "1"
torch.set_num_threads(1)

# Suppress warnings
warnings.filterwarnings('ignore')

# Configure matplotlib for Greek support
matplotlib.use('Agg')
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['font.size'] = 10
plt.rcParams['axes.unicode_minus'] = False

# Codespaces optimization settings
CODESPACES_MODE = True  # Set to True for resource-constrained environments
MAX_TEXT_SIZE_MB = 10 if CODESPACES_MODE else 50  # Reduced for Codespaces
MAX_PROCESS_CHARS = 500000 if CODESPACES_MODE else 1000000  # 500K chars for Codespaces
MAX_SENTENCES_FOR_SENTIMENT = 50 if CODESPACES_MODE else 100  # Limit sentence analysis
SENTIMENT_BATCH_SIZE = 8  # Process sentences in batches


def get_memory_usage():
    """Get current memory usage information"""
    process = psutil.Process(os.getpid())
    memory_info = process.memory_info()
    memory_mb = memory_info.rss / 1024 / 1024
    return f"{memory_mb:.1f} MB"


# Cache expensive resource loading
@st.cache_resource
def load_spacy_model():
    """Load Greek spacy model once and cache it"""
    return spacy.load('el_core_news_sm')


@st.cache_data
def get_stopwords():
    """Cache Greek stopwords set"""
    return set(stopwords.words('greek'))


@st.cache_resource
def load_greek_sentiment_model():
    """Load Greek sentiment analysis model using XLM-RoBERTa (only when needed)"""
    try:
        # Check available memory before loading
        process = psutil.Process(os.getpid())
        memory_mb = process.memory_info().rss / 1024 / 1024

        if memory_mb > 1500 and CODESPACES_MODE:  # Warning if already using > 1.5GB
            st.warning("âš ï¸ High memory usage detected. Sentiment analysis may be slow or unavailable.")

        with st.spinner("Loading sentiment analysis model (this may take a moment)..."):
            # Using a multilingual model that works well with Greek
            model_name = "cardiffnlp/twitter-xlm-roberta-base-sentiment"
            tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)
            model = AutoModelForSequenceClassification.from_pretrained(model_name)
            # Force CPU usage and reduce batch size to save memory
            sentiment_analyzer = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer,
                                         max_length=512, truncation=True, device=-1,
                                         batch_size=1)  # Process one at a time to save memory

            # Force garbage collection after loading
            gc.collect()

            return sentiment_analyzer
    except Exception as e:
        st.error(f"Error loading sentiment model: {e}")
        st.info("ğŸ’¡ Try restarting the app or using a machine with more available memory.")
        return None


# Greek emotion lexicon (simplified)
GREEK_EMOTION_LEXICON = {
    'Ï‡Î±ÏÎ¬': {'joy': 1.0},
    'ÎµÏ…Ï„Ï…Ï‡Î¯Î±': {'joy': 1.0},
    'Ï‡Î±ÏÎ¿ÏÎ¼ÎµÎ½Î¿Ï‚': {'joy': 0.8},
    'Ï‡Î±ÏÎ¿ÏÎ¼ÎµÎ½Î·': {'joy': 0.8},
    'ÎµÏ…Ï‡Î±ÏÎ¹ÏƒÏ„Î·Î¼Î­Î½Î¿Ï‚': {'joy': 0.7, 'trust': 0.3},
    'ÎµÏ…Ï‡Î±ÏÎ¹ÏƒÏ„Î·Î¼Î­Î½Î·': {'joy': 0.7, 'trust': 0.3},
    'Î»ÏÏ€Î·': {'sadness': 1.0},
    'Î»Ï…Ï€Î·Î¼Î­Î½Î¿Ï‚': {'sadness': 0.8},
    'Î»Ï…Ï€Î·Î¼Î­Î½Î·': {'sadness': 0.8},
    'Î¸Î»Î¯ÏˆÎ·': {'sadness': 0.9},
    'ÏƒÏ„ÎµÎ½Î±Ï‡ÏÏÎ¹Î±': {'sadness': 0.7},
    'Î¸Ï…Î¼ÏŒÏ‚': {'anger': 1.0},
    'Î¸Ï…Î¼Ï‰Î¼Î­Î½Î¿Ï‚': {'anger': 0.8},
    'Î¸Ï…Î¼Ï‰Î¼Î­Î½Î·': {'anger': 0.8},
    'ÎµÎºÎ½ÎµÏ…ÏÎ¹ÏƒÎ¼ÏŒÏ‚': {'anger': 0.6},
    'Î¿ÏÎ³Î®': {'anger': 0.9},
    'Ï†ÏŒÎ²Î¿Ï‚': {'fear': 1.0},
    'Ï†Î¿Î²Î¹ÏƒÎ¼Î­Î½Î¿Ï‚': {'fear': 0.8},
    'Ï†Î¿Î²Î¹ÏƒÎ¼Î­Î½Î·': {'fear': 0.8},
    'Î±Î½Î·ÏƒÏ…Ï‡Î¯Î±': {'fear': 0.6, 'anticipation': 0.3},
    'Ï„ÏÏŒÎ¼Î¿Ï‚': {'fear': 0.9},
    'Î­ÎºÏ€Î»Î·Î¾Î·': {'surprise': 1.0},
    'ÎµÎºÏ€Î»Î·ÎºÏ„Î¿Ï‚': {'surprise': 0.8},
    'Î­ÎºÏ€Î»Î·ÎºÏ„Î·': {'surprise': 0.8},
    'Î±Î·Î´Î¯Î±': {'disgust': 1.0},
    'Î±Ï€Î­Ï‡Î¸ÎµÎ¹Î±': {'disgust': 0.9},
    'ÎµÎ¼Ï€Î¹ÏƒÏ„Î¿ÏƒÏÎ½Î·': {'trust': 1.0},
    'Î±Î¾Î¹Î¿Ï€Î¹ÏƒÏ„Î¯Î±': {'trust': 0.8},
    'Ï€Î¯ÏƒÏ„Î·': {'trust': 0.7},
    'Ï€ÏÎ¿ÏƒÎ¼Î¿Î½Î®': {'anticipation': 1.0},
    'Ï€ÏÎ¿ÏƒÎ´Î¿ÎºÎ¯Î±': {'anticipation': 0.8},
    'Î±Î½Î±Î¼Î¿Î½Î®': {'anticipation': 0.7},
    'Î±Î³Î¬Ï€Î·': {'joy': 0.6, 'trust': 0.4},
    'Î¼Î¯ÏƒÎ¿Ï‚': {'anger': 0.7, 'disgust': 0.3},
    'ÎµÎ»Ï€Î¯Î´Î±': {'anticipation': 0.6, 'joy': 0.4},
    'Î±Ï€ÎµÎ»Ï€Î¹ÏƒÎ¯Î±': {'sadness': 0.6, 'fear': 0.4},
    'ÎµÎ½Î¸Î¿Ï…ÏƒÎ¹Î±ÏƒÎ¼ÏŒÏ‚': {'joy': 0.5, 'anticipation': 0.5},
    'Î±Ï€Î¿Î³Î¿Î®Ï„ÎµÏ…ÏƒÎ·': {'sadness': 0.6, 'anger': 0.4},
}


class TextAnalyzer:
    def __init__(self, text):
        """
        Initialize the TextAnalyzer with Greek text.

        Args:
            text (str): The full text to be analyzed
        """
        # Check text size limit (dynamic based on CODESPACES_MODE)
        max_bytes = MAX_TEXT_SIZE_MB * 1024 * 1024
        if len(text.encode('utf-8')) > max_bytes:
            raise ValueError(f"Text file too large. Maximum size is {MAX_TEXT_SIZE_MB}MB.")

        # Clean and store text
        self.original_text = self.clean_text_for_display(text)

        # Limit text length for processing (use dynamic limit)
        self.process_text = self.original_text[:MAX_PROCESS_CHARS] if len(self.original_text) > MAX_PROCESS_CHARS else self.original_text

        # Lazy initialization
        self._nlp = None
        self._doc = None
        self._processed_text = None
        self._tokens = None
        self._sentences = None
        self._cleaned_tokens = None
        self._pos_counts = None
        self._sentiment_analyzer = None

        # Add truncation warning if needed
        if len(self.original_text) > MAX_PROCESS_CHARS:
            self.truncation_warning = f"âš ï¸ Text truncated to {MAX_PROCESS_CHARS:,} characters for processing efficiency."
        else:
            self.truncation_warning = None

    @property
    def sentiment_analyzer(self):
        """Lazy loading of sentiment analyzer"""
        if self._sentiment_analyzer is None:
            self._sentiment_analyzer = load_greek_sentiment_model()
        return self._sentiment_analyzer

    @property
    def nlp(self):
        """Lazy loading of spacy model"""
        if self._nlp is None:
            self._nlp = load_spacy_model()
        return self._nlp

    @property
    def doc(self):
        """Lazy processing of text with spacy"""
        if self._doc is None:
            self._doc = self.nlp(self.process_text)
        return self._doc

    @property
    def processed_text(self):
        """Lazy text preprocessing"""
        if self._processed_text is None:
            self._processed_text = self.preprocess_text(self.process_text)
        return self._processed_text

    @property
    def tokens(self):
        """Lazy tokenization"""
        if self._tokens is None:
            self._tokens = word_tokenize(self.processed_text)
        return self._tokens

    @property
    def sentences(self):
        """Lazy sentence tokenization"""
        if self._sentences is None:
            self._sentences = sent_tokenize(self.process_text)
        return self._sentences

    @staticmethod
    def clean_text_for_display(text):
        """Clean text of problematic characters"""
        text = text.replace('\r\n', '\n').replace('\r', '\n')
        text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F-\x9F]', '', text)
        return text

    @staticmethod
    def preprocess_text(text):
        """Preprocess Greek text: lowercase, remove non-Greek chars, normalize whitespace"""
        text = text.lower()
        text = re.sub(r'[^Î±-Ï‰Î¬-ÏÏŠÏ‹ÎÎ°\s]', '', text)
        text = ' '.join(text.split())
        return text

    def get_cleaned_tokens(self):
        """Get tokens with stopwords removed (cached)"""
        if self._cleaned_tokens is None:
            stop_words = get_stopwords()
            self._cleaned_tokens = [token for token in self.tokens if token not in stop_words]
        return self._cleaned_tokens

    def get_word_frequencies(self, top_n=20):
        """Get word frequency distribution"""
        cleaned_tokens = self.get_cleaned_tokens()
        freq_dist = FreqDist(cleaned_tokens)

        freq_df = pd.DataFrame(
            freq_dist.most_common(top_n),
            columns=['Word', 'Frequency']
        )

        return freq_df

    def get_pos_counts(self):
        """Get POS counts (cached)"""
        if self._pos_counts is None:
            self._pos_counts = Counter([token.pos_ for token in self.doc])
        return self._pos_counts

    def pos_analysis(self):
        """Analyze parts of speech"""
        pos_counts = self.get_pos_counts()

        pos_df = pd.DataFrame(
            list(pos_counts.items()),
            columns=['POS Tag', 'Frequency']
        ).sort_values('Frequency', ascending=False)

        pos_df.index = range(1, len(pos_df) + 1)

        return pos_df

    def basic_statistics(self):
        """Calculate basic text statistics"""
        token_count = len(self.tokens)
        sentence_count = len(self.sentences)

        return {
            'Total Words': token_count,
            'Total Sentences': sentence_count,
            'Average Word Length': np.mean([len(word) for word in self.tokens]) if self.tokens else 0,
            'Average Sentence Length': token_count / sentence_count if sentence_count > 0 else 0
        }

    def sentiment_analysis(self):
        """
        Analyze sentiment using Greek-capable transformer model.

        Returns:
            dict: Dictionary containing sentiment polarity and subjectivity
        """
        if self.sentiment_analyzer is None:
            return {'Overall Polarity': 0.0, 'Overall Subjectivity': 0.5, 'error': True}

        try:
            # Truncate text if too long
            text_to_analyze = self.original_text[:512]
            result = self.sentiment_analyzer(text_to_analyze)[0]

            # Map labels to polarity
            label_map = {
                'positive': 0.5,
                'Positive': 0.5,
                'POSITIVE': 0.5,
                'neutral': 0.0,
                'Neutral': 0.0,
                'NEUTRAL': 0.0,
                'negative': -0.5,
                'Negative': -0.5,
                'NEGATIVE': -0.5
            }

            polarity = label_map.get(result['label'], 0.0)
            confidence = result['score']

            # Adjust polarity based on confidence
            polarity = polarity * confidence

            return {
                'Overall Polarity': polarity,
                'Overall Subjectivity': confidence,
                'label': result['label'],
                'confidence': confidence
            }
        except Exception as e:
            return {'Overall Polarity': 0.0, 'Overall Subjectivity': 0.5, 'error': str(e)}

    def sentence_sentiment_analysis(self):
        """
        Analyze sentiment for each sentence with sampling for large texts.

        Returns:
            pd.DataFrame: DataFrame containing sentences and their sentiment scores
        """
        if self.sentiment_analyzer is None:
            return pd.DataFrame()

        data = {'Sentence': [], 'Polarity': [], 'Subjectivity': [], 'Sentiment': []}

        # Sample sentences if there are too many (for Codespaces efficiency)
        sentences_to_analyze = self.sentences
        total_sentences = len(sentences_to_analyze)
        sampled = False

        if total_sentences > MAX_SENTENCES_FOR_SENTIMENT:
            # Intelligently sample: take first, last, and evenly spaced middle sentences
            step = total_sentences // MAX_SENTENCES_FOR_SENTIMENT
            indices = list(range(0, total_sentences, step))[:MAX_SENTENCES_FOR_SENTIMENT]
            sentences_to_analyze = [self.sentences[i] for i in indices]
            sampled = True

        # Create progress bar if processing many sentences
        if len(sentences_to_analyze) > 10:
            progress_bar = st.progress(0)
            status_text = st.empty()
        else:
            progress_bar = None
            status_text = None

        for idx, sentence in enumerate(sentences_to_analyze):
            try:
                # Update progress
                if progress_bar is not None:
                    progress = (idx + 1) / len(sentences_to_analyze)
                    progress_bar.progress(progress)
                    status_text.text(f'Analyzing sentence {idx + 1} of {len(sentences_to_analyze)}...')

                # Truncate sentence if too long
                sentence_text = sentence[:512]
                result = self.sentiment_analyzer(sentence_text)[0]

                # Map labels to polarity
                label_map = {
                    'positive': 0.5, 'Positive': 0.5, 'POSITIVE': 0.5,
                    'neutral': 0.0, 'Neutral': 0.0, 'NEUTRAL': 0.0,
                    'negative': -0.5, 'Negative': -0.5, 'NEGATIVE': -0.5
                }

                polarity = label_map.get(result['label'], 0.0) * result['score']
                confidence = result['score']

                # Determine sentiment label
                if polarity > 0.1:
                    sentiment = 'Positive'
                elif polarity < -0.1:
                    sentiment = 'Negative'
                else:
                    sentiment = 'Neutral'

                data['Sentence'].append(sentence)
                data['Polarity'].append(polarity)
                data['Subjectivity'].append(confidence)
                data['Sentiment'].append(sentiment)

            except Exception as e:
                data['Sentence'].append(sentence)
                data['Polarity'].append(0.0)
                data['Subjectivity'].append(0.5)
                data['Sentiment'].append('Neutral')

            # Periodic garbage collection for memory efficiency
            if idx % 20 == 0:
                gc.collect()

        # Clear progress indicators
        if progress_bar is not None:
            progress_bar.empty()
            status_text.empty()

        df = pd.DataFrame(data)

        # Add note about sampling if applied
        if sampled:
            st.info(f"â„¹ï¸ Analyzed {len(sentences_to_analyze)} sampled sentences out of {total_sentences} total sentences for efficiency.")

        return df

    def emotion_analysis_from_lexicon(self, text):
        """
        Analyze emotions using Greek lexicon.

        Args:
            text (str): Text to analyze

        Returns:
            dict: Emotion scores
        """
        tokens = word_tokenize(text.lower())
        emotion_scores = {
            'joy': 0.0, 'sadness': 0.0, 'anger': 0.0, 'fear': 0.0,
            'surprise': 0.0, 'disgust': 0.0, 'trust': 0.0, 'anticipation': 0.0
        }

        total_emotional_words = 0

        for token in tokens:
            if token in GREEK_EMOTION_LEXICON:
                emotions = GREEK_EMOTION_LEXICON[token]
                for emotion, score in emotions.items():
                    emotion_scores[emotion] += score
                total_emotional_words += 1

        # Normalize scores
        if total_emotional_words > 0:
            for emotion in emotion_scores:
                emotion_scores[emotion] = emotion_scores[emotion] / total_emotional_words

        return emotion_scores

    def emotion_analysis(self):
        """
        Analyze emotions in the text.

        Returns:
            dict: Dictionary containing emotion scores
        """
        return self.emotion_analysis_from_lexicon(self.original_text)

    def sentence_emotion_analysis(self):
        """
        Analyze emotions for each sentence with sampling for large texts.

        Returns:
            pd.DataFrame: DataFrame containing sentences and their emotion scores
        """
        sentence_emotions = []

        # Sample sentences if there are too many
        sentences_to_analyze = self.sentences
        total_sentences = len(sentences_to_analyze)
        sampled = False

        if total_sentences > MAX_SENTENCES_FOR_SENTIMENT:
            step = total_sentences // MAX_SENTENCES_FOR_SENTIMENT
            indices = list(range(0, total_sentences, step))[:MAX_SENTENCES_FOR_SENTIMENT]
            sentences_to_analyze = [self.sentences[i] for i in indices]
            sampled = True

        for sentence in sentences_to_analyze:
            emotions = self.emotion_analysis_from_lexicon(sentence)

            emotion_dict = {
                'Sentence': sentence,
                'Joy': emotions['joy'],
                'Sadness': emotions['sadness'],
                'Anger': emotions['anger'],
                'Fear': emotions['fear'],
                'Surprise': emotions['surprise'],
                'Disgust': emotions['disgust'],
                'Trust': emotions['trust'],
                'Anticipation': emotions['anticipation']
            }

            # Find dominant emotion
            emotion_scores = {k: v for k, v in emotion_dict.items() if k != 'Sentence'}
            if any(emotion_scores.values()):
                dominant_emotion = max(emotion_scores, key=emotion_scores.get)
                emotion_dict['Dominant Emotion'] = dominant_emotion
            else:
                emotion_dict['Dominant Emotion'] = 'Neutral'

            sentence_emotions.append(emotion_dict)

        df = pd.DataFrame(sentence_emotions)

        # Add note about sampling if applied
        if sampled:
            st.info(f"â„¹ï¸ Analyzed {len(sentences_to_analyze)} sampled sentences out of {total_sentences} total sentences for efficiency.")

        return df

    def topic_modeling_lda(self, n_topics=5, n_top_words=10, method='lda'):
        """
        Perform topic modeling using LDA or NMF.

        Args:
            n_topics (int): Number of topics to extract
            n_top_words (int): Number of top words per topic
            method (str): 'lda' or 'nmf'

        Returns:
            dict: Dictionary containing topic information
        """
        if len(self.sentences) < 3:
            return None

        documents = self.sentences
        stop_words = list(get_stopwords())

        # Create document-term matrix
        if method == 'lda':
            vectorizer = CountVectorizer(
                max_df=0.85,
                min_df=2,
                stop_words=stop_words,
                max_features=1000,
                token_pattern=r'[Î±-Ï‰Î¬-ÏÏŠÏ‹ÎÎ°]+'
            )
        else:  # nmf
            vectorizer = TfidfVectorizer(
                max_df=0.85,
                min_df=2,
                stop_words=stop_words,
                max_features=1000,
                token_pattern=r'[Î±-Ï‰Î¬-ÏÏŠÏ‹ÎÎ°]+'
            )

        try:
            doc_term_matrix = vectorizer.fit_transform(documents)

            # Fit the model with reduced iterations for Codespaces
            if method == 'lda':
                max_iter_lda = 10 if CODESPACES_MODE else 20
                model = LatentDirichletAllocation(
                    n_components=n_topics,
                    random_state=42,
                    max_iter=max_iter_lda,
                    learning_method='online',
                    n_jobs=1  # Single thread for stability
                )
            else:  # nmf
                max_iter_nmf = 100 if CODESPACES_MODE else 200
                model = NMF(
                    n_components=n_topics,
                    random_state=42,
                    max_iter=max_iter_nmf,
                    init='nndsvda'  # Faster initialization
                )

            with st.spinner(f'Training {method.upper()} model...'):
                doc_topic_dist = model.fit_transform(doc_term_matrix)

            # Force garbage collection after model training
            gc.collect()

            # Get feature names
            feature_names = vectorizer.get_feature_names_out()

            # Extract topics
            topics = {}
            for topic_idx, topic in enumerate(model.components_):
                top_indices = topic.argsort()[-n_top_words:][::-1]
                top_words = [feature_names[i] for i in top_indices]
                top_weights = [topic[i] for i in top_indices]
                topics[f'Topic {topic_idx + 1}'] = {
                    'words': top_words,
                    'weights': top_weights
                }

            # Get dominant topic for each document
            dominant_topics = []
            for i, doc_dist in enumerate(doc_topic_dist):
                dominant_topic = doc_dist.argmax()
                dominant_topics.append({
                    'Sentence': documents[i][:100] + '...' if len(documents[i]) > 100 else documents[i],
                    'Dominant Topic': f'Topic {dominant_topic + 1}',
                    'Topic Weight': doc_dist[dominant_topic]
                })

            return {
                'topics': topics,
                'doc_topic_dist': doc_topic_dist,
                'dominant_topics': pd.DataFrame(dominant_topics),
                'model': model,
                'vectorizer': vectorizer
            }

        except Exception as e:
            return None


def create_safe_plot(figsize=(10, 6)):
    """Create a matplotlib figure with safe settings and memory cleanup"""
    plt.close('all')  # Close all previous figures
    plt.clf()
    fig, ax = plt.subplots(figsize=figsize)
    return fig, ax


@st.cache_resource
def analyze_text(text):
    """Cache the text analyzer initialization"""
    return TextAnalyzer(text)


def main():
    st.title('Î•ÏÎ³Î±Î»ÎµÎ¯Î¿ Î‘Î½Î¬Î»Ï…ÏƒÎ·Ï‚ ÎšÎµÎ¹Î¼Î­Î½Î¿Ï…')  # Text Analysis Tool in Greek

    # Show Codespaces optimization notice
    if CODESPACES_MODE:
        st.info("ğŸš€ Running in optimized mode for GitHub Codespaces. Text size and processing limits are adjusted for efficient performance.")

    with st.sidebar:
        st.header("Î•Ï€Î¹Î»Î¿Î³Î­Ï‚ Î‘Î½Î¬Î»Ï…ÏƒÎ·Ï‚")  # Analysis Options
        analysis_tab = st.radio(
            "Î•Ï€Î¹Î»Î­Î¾Ï„Îµ Î¤ÏÏ€Î¿ Î‘Î½Î¬Î»Ï…ÏƒÎ·Ï‚",  # Select Analysis Type
            ["Î’Î±ÏƒÎ¹ÎºÎ® Î‘Î½Î¬Î»Ï…ÏƒÎ·", "Î‘Î½Î¬Î»Ï…ÏƒÎ· Î†Ï€Î¿ÏˆÎ·Ï‚", "Î‘Î½Î¬Î»Ï…ÏƒÎ· Î£Ï…Î½Î±Î¹ÏƒÎ¸Î·Î¼Î¬Ï„Ï‰Î½", "Î˜ÎµÎ¼Î±Ï„Î¹ÎºÎ® ÎœÎ¿Î½Ï„ÎµÎ»Î¿Ï€Î¿Î¯Î·ÏƒÎ·"]
        )

        # Memory usage display
        st.divider()
        st.subheader("ğŸ“Š Î§ÏÎ®ÏƒÎ· ÎœÎ½Î®Î¼Î·Ï‚")
        memory_usage = get_memory_usage()
        memory_color = "normal"
        try:
            mem_mb = float(memory_usage.split()[0])
            if mem_mb > 1500:
                memory_color = "inverse"
        except:
            pass
        st.metric("ÎœÎ½Î®Î¼Î· Î•Ï†Î±ÏÎ¼Î¿Î³Î®Ï‚", memory_usage, delta="High" if memory_color == "inverse" else None)

        # Show limits
        if CODESPACES_MODE:
            st.divider()
            st.caption(f"ğŸ“ Max file size: {MAX_TEXT_SIZE_MB}MB")
            st.caption(f"ğŸ“ Max processing: {MAX_PROCESS_CHARS:,} chars")

    # File upload
    uploaded_file = st.file_uploader("Î•Ï€Î¹Î»Î­Î¾Ï„Îµ Î­Î½Î± Î±ÏÏ‡ÎµÎ¯Î¿ ÎºÎµÎ¹Î¼Î­Î½Î¿Ï…", type=['txt'])

    if uploaded_file is not None:
        try:
            # Read the file
            text = uploaded_file.getvalue().decode("utf-8")

            # Create analyzer with caching (this will check file size)
            analyzer = analyze_text(text)

            # Show truncation warning if applicable
            if analyzer.truncation_warning:
                st.warning(analyzer.truncation_warning)

            with st.expander("Î ÏÎ¿Î²Î¿Î»Î® Î ÎµÏÎ¹ÎµÏ‡Î¿Î¼Î­Î½Î¿Ï… ÎšÎµÎ¹Î¼Î­Î½Î¿Ï…", expanded=False):
                st.text_area('Î ÎµÏÎ¹ÎµÏ‡ÏŒÎ¼ÎµÎ½Î¿ ÎšÎµÎ¹Î¼Î­Î½Î¿Ï…', text, height=200)

            # Display basic statistics
            stats = analyzer.basic_statistics()
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("Î£ÏÎ½Î¿Î»Î¿ Î›Î­Î¾ÎµÏ‰Î½", f"{stats['Total Words']:.0f}")
            with col2:
                st.metric("Î£ÏÎ½Î¿Î»Î¿ Î ÏÎ¿Ï„Î¬ÏƒÎµÏ‰Î½", f"{stats['Total Sentences']:.0f}")
            with col3:
                st.metric("ÎœÎ­ÏƒÎ¿ ÎœÎ®ÎºÎ¿Ï‚ Î›Î­Î¾Î·Ï‚", f"{stats['Average Word Length']:.2f}")
            with col4:
                st.metric("ÎœÎ­ÏƒÎ¿ ÎœÎ®ÎºÎ¿Ï‚ Î ÏÏŒÏ„Î±ÏƒÎ·Ï‚", f"{stats['Average Sentence Length']:.2f}")

            if analysis_tab == "Î’Î±ÏƒÎ¹ÎºÎ® Î‘Î½Î¬Î»Ï…ÏƒÎ·":
                # Word Frequency
                st.header('Î£Ï…Ï‡Î½ÏŒÏ„Î·Ï„ÎµÏ‚ Î›Î­Î¾ÎµÏ‰Î½')
                col1, col2 = st.columns([1, 2])

                with col1:
                    top_n = st.slider('Î•Ï€Î¹Î»Î­Î¾Ï„Îµ Î±ÏÎ¹Î¸Î¼ÏŒ ÎºÎ¿ÏÏ…Ï†Î±Î¯Ï‰Î½ Î»Î­Î¾ÎµÏ‰Î½', 5, 50, 20)
                    freq_df = analyzer.get_word_frequencies(top_n)
                    freq_df.index = freq_df.index + 1
                    st.dataframe(freq_df, width='stretch')

                with col2:
                    try:
                        fig, ax = create_safe_plot()
                        freq_data = freq_df.sort_values('Frequency', ascending=True).tail(15)
                        sns.barplot(x='Frequency', y='Word', data=freq_data, ax=ax)
                        ax.set_title('Î£Ï…Ï‡Î½ÏŒÏ„Î·Ï„ÎµÏ‚ Î›Î­Î¾ÎµÏ‰Î½')
                        plt.tight_layout()
                        st.pyplot(fig)
                        plt.close(fig)
                    except Exception as e:
                        st.error(f"Î£Ï†Î¬Î»Î¼Î± Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î±Ï‚ Î³ÏÎ±Ï†Î®Î¼Î±Ï„Î¿Ï‚: {str(e)}")

                # POS Tag Frequencies
                st.header('Î‘Î½Î¬Î»Ï…ÏƒÎ· ÎœÎµÏÏÎ½ Ï„Î¿Ï… Î›ÏŒÎ³Î¿Ï…')
                col1, col2 = st.columns([1, 2])

                with col1:
                    pos_df = analyzer.pos_analysis()
                    st.dataframe(pos_df, width='stretch')

                with col2:
                    try:
                        fig, ax = create_safe_plot()
                        pos_data = pos_df.sort_values('Frequency', ascending=True).tail(10)
                        sns.barplot(x='Frequency', y='POS Tag', data=pos_data, ax=ax)
                        ax.set_title('ÎšÎ¿ÏÏ…Ï†Î±Î¯ÎµÏ‚ 10 Î£Ï…Ï‡Î½ÏŒÏ„Î·Ï„ÎµÏ‚ ÎœÎµÏÏÎ½ Ï„Î¿Ï… Î›ÏŒÎ³Î¿Ï…')
                        plt.tight_layout()
                        st.pyplot(fig)
                        plt.close(fig)
                    except Exception as e:
                        st.error(f"Î£Ï†Î¬Î»Î¼Î± Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î±Ï‚ Î³ÏÎ±Ï†Î®Î¼Î±Ï„Î¿Ï‚: {str(e)}")

                # Tagged Text
                with st.expander("Î ÏÎ¿Î²Î¿Î»Î® Î£Î·Î¼ÎµÎ¹Ï‰Î¼Î­Î½Î¿Ï… ÎšÎµÎ¹Î¼Î­Î½Î¿Ï…", expanded=False):
                    try:
                        pos_text = ""
                        for sent in analyzer.doc.sents:
                            for token in sent:
                                pos_text += f"{token.text}[{token.pos_}] "
                            pos_text += "\n\n"
                        st.text_area('Î£Î·Î¼ÎµÎ¹Ï‰Î¼Î­Î½Î¿ ÎšÎµÎ¯Î¼ÎµÎ½Î¿', pos_text, height=300)
                    except Exception as e:
                        st.error(f"Î£Ï†Î¬Î»Î¼Î± ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î±Ï‚ ÎºÎµÎ¹Î¼Î­Î½Î¿Ï…: {str(e)}")

            elif analysis_tab == "Î‘Î½Î¬Î»Ï…ÏƒÎ· Î†Ï€Î¿ÏˆÎ·Ï‚":
                st.header('Î‘Î½Î¬Î»Ï…ÏƒÎ· Î†Ï€Î¿ÏˆÎ·Ï‚')

                # Overall Sentiment
                sentiment = analyzer.sentiment_analysis()

                if 'error' not in sentiment or not sentiment['error']:
                    polarity = sentiment['Overall Polarity']
                    subjectivity = sentiment['Overall Subjectivity']

                    col1, col2 = st.columns(2)

                    with col1:
                        st.subheader("Î Î¿Î»Î¹ÎºÏŒÏ„Î·Ï„Î± Î†Ï€Î¿ÏˆÎ·Ï‚")
                        try:
                            fig, ax = create_safe_plot()
                            ax.set_xlim(-1, 1)
                            ax.set_ylim(0, 0.1)
                            ax.axvline(x=0, color='gray', linestyle='-', alpha=0.3)
                            ax.axvspan(-1, -0.1, color='red', alpha=0.2)
                            ax.axvspan(-0.1, 0.1, color='gray', alpha=0.2)
                            ax.axvspan(0.1, 1, color='green', alpha=0.2)
                            ax.scatter(polarity, 0.05, s=300, color='blue', zorder=5)

                            ax.set_yticks([])
                            ax.set_ylabel('')
                            ax.set_xticks([-1, -0.5, 0, 0.5, 1])
                            ax.set_xticklabels(['Î‘ÏÎ½Î·Ï„Î¹ÎºÏŒ', 'ÎšÎ¬Ï€Ï‰Ï‚\nÎ‘ÏÎ½Î·Ï„Î¹ÎºÏŒ', 'ÎŸÏ…Î´Î­Ï„ÎµÏÎ¿', 'ÎšÎ¬Ï€Ï‰Ï‚\nÎ˜ÎµÏ„Î¹ÎºÏŒ', 'Î˜ÎµÏ„Î¹ÎºÏŒ'])
                            st.pyplot(fig)
                            plt.close(fig)
                        except Exception as e:
                            st.error(f"Î£Ï†Î¬Î»Î¼Î± Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î±Ï‚ Î³ÏÎ±Ï†Î®Î¼Î±Ï„Î¿Ï‚: {str(e)}")

                        st.metric("Î’Î±Î¸Î¼Î¿Î»Î¿Î³Î¯Î± Î Î¿Î»Î¹ÎºÏŒÏ„Î·Ï„Î±Ï‚", f"{polarity:.3f}",
                                  delta=None if -0.1 <= polarity <= 0.1 else ("Î˜ÎµÏ„Î¹ÎºÏŒ" if polarity > 0 else "Î‘ÏÎ½Î·Ï„Î¹ÎºÏŒ"))

                    with col2:
                        st.subheader("Î¥Ï€Î¿ÎºÎµÎ¹Î¼ÎµÎ½Î¹ÎºÏŒÏ„Î·Ï„Î±")
                        try:
                            fig, ax = create_safe_plot()
                            ax.set_xlim(0, 1)
                            ax.set_ylim(0, 0.1)
                            ax.axvspan(0, 0.3, color='lightblue', alpha=0.3)
                            ax.axvspan(0.3, 0.7, color='lightgreen', alpha=0.3)
                            ax.axvspan(0.7, 1, color='lightyellow', alpha=0.3)
                            ax.scatter(subjectivity, 0.05, s=300, color='blue', zorder=5)

                            ax.set_yticks([])
                            ax.set_ylabel('')
                            ax.set_xticks([0, 0.3, 0.7, 1])
                            ax.set_xticklabels(['Î‘Î½Ï„Î¹ÎºÎµÎ¹Î¼ÎµÎ½Î¹ÎºÏŒ', 'ÎšÎ¬Ï€Ï‰Ï‚\nÎ‘Î½Ï„Î¹ÎºÎµÎ¹Î¼ÎµÎ½Î¹ÎºÏŒ', 'ÎšÎ¬Ï€Ï‰Ï‚\nÎ¥Ï€Î¿ÎºÎµÎ¹Î¼ÎµÎ½Î¹ÎºÏŒ', 'Î¥Ï€Î¿ÎºÎµÎ¹Î¼ÎµÎ½Î¹ÎºÏŒ'])
                            st.pyplot(fig)
                            plt.close(fig)
                        except Exception as e:
                            st.error(f"Î£Ï†Î¬Î»Î¼Î± Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î±Ï‚ Î³ÏÎ±Ï†Î®Î¼Î±Ï„Î¿Ï‚: {str(e)}")

                        st.metric("Î’Î±Î¸Î¼Î¿Î»Î¿Î³Î¯Î± Î¥Ï€Î¿ÎºÎµÎ¹Î¼ÎµÎ½Î¹ÎºÏŒÏ„Î·Ï„Î±Ï‚", f"{subjectivity:.3f}",
                                  delta="Î ÏÎ±Î³Î¼Î±Ï„Î¹ÎºÏŒ" if subjectivity < 0.5 else "Î“Î½Ï‰Î¼Î¹ÎºÏŒ")

                    # Sentence Level Sentiment
                    st.subheader('Î‘Î½Î¬Î»Ï…ÏƒÎ· Î†Ï€Î¿ÏˆÎ·Ï‚ ÏƒÎµ Î•Ï€Î¯Ï€ÎµÎ´Î¿ Î ÏÏŒÏ„Î±ÏƒÎ·Ï‚')
                    sent_df = analyzer.sentence_sentiment_analysis()

                    if not sent_df.empty:
                        col1, col2 = st.columns([2, 1])

                        with col1:
                            try:
                                fig, ax = create_safe_plot()
                                sentiment_counts = sent_df['Sentiment'].value_counts()
                                colors = {'Positive': 'green', 'Neutral': 'gray', 'Negative': 'red'}
                                bars = ax.bar(sentiment_counts.index, sentiment_counts.values,
                                             color=[colors.get(x, 'blue') for x in sentiment_counts.index])
                                ax.set_title('ÎšÎ±Ï„Î±Î½Î¿Î¼Î® Î†Ï€Î¿ÏˆÎ·Ï‚ Î ÏÎ¿Ï„Î¬ÏƒÎµÏ‰Î½')
                                ax.set_ylabel('Î‘ÏÎ¹Î¸Î¼ÏŒÏ‚ Î ÏÎ¿Ï„Î¬ÏƒÎµÏ‰Î½')

                                for i, v in enumerate(sentiment_counts.values):
                                    ax.text(i, v + 0.1, str(v), ha='center')

                                st.pyplot(fig)
                                plt.close(fig)
                            except Exception as e:
                                st.error(f"Î£Ï†Î¬Î»Î¼Î± Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î±Ï‚ Î³ÏÎ±Ï†Î®Î¼Î±Ï„Î¿Ï‚: {str(e)}")

                        with col2:
                            sentiment_counts_dict = sent_df['Sentiment'].value_counts().to_dict()
                            positive_count = sentiment_counts_dict.get('Positive', 0)
                            negative_count = sentiment_counts_dict.get('Negative', 0)
                            neutral_count = sentiment_counts_dict.get('Neutral', 0)
                            total_sentences = len(sent_df)

                            if total_sentences > 0:
                                st.metric("Î˜ÎµÏ„Î¹ÎºÎ­Ï‚ Î ÏÎ¿Ï„Î¬ÏƒÎµÎ¹Ï‚", positive_count, f"{positive_count/total_sentences*100:.1f}%")
                                st.metric("ÎŸÏ…Î´Î­Ï„ÎµÏÎµÏ‚ Î ÏÎ¿Ï„Î¬ÏƒÎµÎ¹Ï‚", neutral_count, f"{neutral_count/total_sentences*100:.1f}%")
                                st.metric("Î‘ÏÎ½Î·Ï„Î¹ÎºÎ­Ï‚ Î ÏÎ¿Ï„Î¬ÏƒÎµÎ¹Ï‚", negative_count, f"{negative_count/total_sentences*100:.1f}%")

                        with st.expander("Î ÏÎ¿Î²Î¿Î»Î® Î›ÎµÏ€Ï„Î¿Î¼ÎµÏÎ¿ÏÏ‚ Î‘Î½Î¬Î»Ï…ÏƒÎ·Ï‚ Î ÏÎ¿Ï„Î¬ÏƒÎµÏ‰Î½", expanded=False):
                            st.dataframe(sent_df, width='stretch')
                else:
                    st.error("Î£Ï†Î¬Î»Î¼Î± Ï†ÏŒÏÏ„Ï‰ÏƒÎ·Ï‚ Î¼Î¿Î½Ï„Î­Î»Î¿Ï… Î±Î½Î¬Î»Ï…ÏƒÎ·Ï‚ Î¬Ï€Î¿ÏˆÎ·Ï‚. Î Î±ÏÎ±ÎºÎ±Î»Ï ÎµÎ»Î­Î³Î¾Ï„Îµ Ï„Î· ÏƒÏÎ½Î´ÎµÏƒÎ® ÏƒÎ±Ï‚ ÏƒÏ„Î¿ Î´Î¹Î±Î´Î¯ÎºÏ„Ï…Î¿.")

            elif analysis_tab == "Î‘Î½Î¬Î»Ï…ÏƒÎ· Î£Ï…Î½Î±Î¹ÏƒÎ¸Î·Î¼Î¬Ï„Ï‰Î½":
                st.header('Î‘Î½Î¬Î»Ï…ÏƒÎ· Î£Ï…Î½Î±Î¹ÏƒÎ¸Î·Î¼Î¬Ï„Ï‰Î½')

                emotions = analyzer.emotion_analysis()

                st.subheader("Î£Ï…Î½Î¿Î»Î¹ÎºÎ® ÎšÎ±Ï„Î±Î½Î¿Î¼Î® Î£Ï…Î½Î±Î¹ÏƒÎ¸Î·Î¼Î¬Ï„Ï‰Î½")

                emotion_df = pd.DataFrame({
                    'Î£Ï…Î½Î±Î¯ÏƒÎ¸Î·Î¼Î±': ['Î§Î±ÏÎ¬', 'Î›ÏÏ€Î·', 'Î˜Ï…Î¼ÏŒÏ‚', 'Î¦ÏŒÎ²Î¿Ï‚', 'ÎˆÎºÏ€Î»Î·Î¾Î·', 'Î‘Î·Î´Î¯Î±', 'Î•Î¼Ï€Î¹ÏƒÏ„Î¿ÏƒÏÎ½Î·', 'Î ÏÎ¿ÏƒÎ¼Î¿Î½Î®'],
                    'Score': [emotions['joy'], emotions['sadness'], emotions['anger'], emotions['fear'],
                             emotions['surprise'], emotions['disgust'], emotions['trust'], emotions['anticipation']]
                }).sort_values('Score', ascending=False)

                emotion_colors = {
                    'Î§Î±ÏÎ¬': '#FFCC00',
                    'Î•Î¼Ï€Î¹ÏƒÏ„Î¿ÏƒÏÎ½Î·': '#4CAF50',
                    'Î ÏÎ¿ÏƒÎ¼Î¿Î½Î®': '#FF9800',
                    'ÎˆÎºÏ€Î»Î·Î¾Î·': '#8E44AD',
                    'Î›ÏÏ€Î·': '#3498DB',
                    'Î¦ÏŒÎ²Î¿Ï‚': '#607D8B',
                    'Î˜Ï…Î¼ÏŒÏ‚': '#F44336',
                    'Î‘Î·Î´Î¯Î±': '#795548'
                }

                colors = [emotion_colors.get(emotion, '#CCCCCC') for emotion in emotion_df['Î£Ï…Î½Î±Î¯ÏƒÎ¸Î·Î¼Î±']]

                try:
                    fig, ax = create_safe_plot()
                    bars = ax.barh(emotion_df['Î£Ï…Î½Î±Î¯ÏƒÎ¸Î·Î¼Î±'], emotion_df['Score'], color=colors)
                    ax.set_xlabel('Î’Î±Î¸Î¼Î¿Î»Î¿Î³Î¯Î±')
                    ax.set_title('Î‘Î½Î¬Î»Ï…ÏƒÎ· Î£Ï…Î½Î±Î¹ÏƒÎ¸Î·Î¼Î¬Ï„Ï‰Î½')

                    for i, bar in enumerate(bars):
                        width = bar.get_width()
                        if width > 0:
                            label_x_pos = width if width > 0.02 else 0.02
                            ax.text(label_x_pos, bar.get_y() + bar.get_height()/2, f'{width:.3f}',
                                    va='center', ha='left' if width <= 0.02 else 'right',
                                    color='black' if width <= 0.02 else 'white')

                    st.pyplot(fig)
                    plt.close(fig)
                except Exception as e:
                    st.error(f"Î£Ï†Î¬Î»Î¼Î± Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î±Ï‚ Î³ÏÎ±Ï†Î®Î¼Î±Ï„Î¿Ï‚: {str(e)}")

                if any(emotions.values()):
                    dominant_emotion = max(emotions, key=emotions.get)
                    dominant_score = emotions[dominant_emotion]

                    emotion_greek_map = {
                        'joy': 'Î§Î±ÏÎ¬', 'sadness': 'Î›ÏÏ€Î·', 'anger': 'Î˜Ï…Î¼ÏŒÏ‚', 'fear': 'Î¦ÏŒÎ²Î¿Ï‚',
                        'surprise': 'ÎˆÎºÏ€Î»Î·Î¾Î·', 'disgust': 'Î‘Î·Î´Î¯Î±', 'trust': 'Î•Î¼Ï€Î¹ÏƒÏ„Î¿ÏƒÏÎ½Î·', 'anticipation': 'Î ÏÎ¿ÏƒÎ¼Î¿Î½Î®'
                    }

                    st.subheader("ÎšÏ…ÏÎ¯Î±ÏÏ‡Î¿ Î£Ï…Î½Î±Î¯ÏƒÎ¸Î·Î¼Î±")
                    st.markdown(f"Î¤Î¿ ÎºÏ…ÏÎ¯Î±ÏÏ‡Î¿ ÏƒÏ…Î½Î±Î¯ÏƒÎ¸Î·Î¼Î± ÏƒÎµ Î±Ï…Ï„ÏŒ Ï„Î¿ ÎºÎµÎ¯Î¼ÎµÎ½Î¿ ÎµÎ¯Î½Î±Î¹: **{emotion_greek_map.get(dominant_emotion, dominant_emotion)}** (Î’Î±Î¸Î¼Î¿Î»Î¿Î³Î¯Î±: {dominant_score:.3f})")
                else:
                    st.info("Î”ÎµÎ½ ÎµÎ½Ï„Î¿Ï€Î¯ÏƒÏ„Î·ÎºÎ±Î½ ÏƒÎ·Î¼Î±Î½Ï„Î¹ÎºÎ¬ ÏƒÏ…Î½Î±Î¹ÏƒÎ¸Î®Î¼Î±Ï„Î± ÏƒÏ„Î¿ ÎºÎµÎ¯Î¼ÎµÎ½Î¿.")

                # Emotion radar chart
                st.subheader("Î ÏÎ¿Ï†Î¯Î» Î£Ï…Î½Î±Î¹ÏƒÎ¸Î·Î¼Î¬Ï„Ï‰Î½")

                try:
                    categories = ['Î§Î±ÏÎ¬', 'Î›ÏÏ€Î·', 'Î˜Ï…Î¼ÏŒÏ‚', 'Î¦ÏŒÎ²Î¿Ï‚', 'ÎˆÎºÏ€Î»Î·Î¾Î·', 'Î‘Î·Î´Î¯Î±', 'Î•Î¼Ï€Î¹ÏƒÏ„Î¿ÏƒÏÎ½Î·', 'Î ÏÎ¿ÏƒÎ¼Î¿Î½Î®']
                    values = [emotions['joy'], emotions['sadness'], emotions['anger'], emotions['fear'],
                             emotions['surprise'], emotions['disgust'], emotions['trust'], emotions['anticipation']]

                    if any(values):
                        angles = np.linspace(0, 2*np.pi, len(categories), endpoint=False).tolist()
                        values = values + [values[0]]
                        angles = angles + [angles[0]]
                        categories = categories + [categories[0]]

                        fig = plt.figure(figsize=(8, 8))
                        ax = fig.add_subplot(111, polar=True)

                        ax.plot(angles, values, 'o-', linewidth=2)
                        ax.fill(angles, values, alpha=0.25)

                        ax.set_xticks(angles[:-1])
                        ax.set_xticklabels(categories[:-1])

                        max_value = max(values)
                        ax.set_ylim(0, max_value * 1.1 if max_value > 0 else 0.1)
                        ax.grid(True)

                        st.pyplot(fig)
                        plt.close(fig)
                    else:
                        st.info("Î”ÎµÎ½ ÎµÎ½Ï„Î¿Ï€Î¯ÏƒÏ„Î·ÎºÎ±Î½ ÏƒÎ·Î¼Î±Î½Ï„Î¹ÎºÎ¬ ÏƒÏ…Î½Î±Î¹ÏƒÎ¸Î®Î¼Î±Ï„Î± ÏƒÏ„Î¿ ÎºÎµÎ¯Î¼ÎµÎ½Î¿.")
                except Exception as e:
                    st.error(f"Î£Ï†Î¬Î»Î¼Î± Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î±Ï‚ Î³ÏÎ±Ï†Î®Î¼Î±Ï„Î¿Ï‚: {str(e)}")

                # Sentence-level emotion analysis
                st.subheader("Î‘Î½Î¬Î»Ï…ÏƒÎ· Î£Ï…Î½Î±Î¹ÏƒÎ¸Î·Î¼Î¬Ï„Ï‰Î½ ÏƒÎµ Î•Ï€Î¯Ï€ÎµÎ´Î¿ Î ÏÏŒÏ„Î±ÏƒÎ·Ï‚")
                try:
                    sentence_emotions = analyzer.sentence_emotion_analysis()

                    emotion_columns = ['Joy', 'Sadness', 'Anger', 'Fear', 'Surprise', 'Disgust', 'Trust', 'Anticipation']

                    display_limit = min(20, len(sentence_emotions))
                    heatmap_data = sentence_emotions.iloc[:display_limit][emotion_columns]

                    sentence_labels = [f"Î {i+1}: {s[:30]}..." if len(s) > 30 else f"Î {i+1}: {s}"
                            for i, s in enumerate(sentence_emotions['Sentence'].iloc[:display_limit])]

                    if not heatmap_data.empty and heatmap_data.sum().sum() > 0:
                        fig, ax = plt.subplots(figsize=(12, max(6, display_limit * 0.4)))

                        # Rename columns to Greek for heatmap
                        heatmap_data_greek = heatmap_data.copy()
                        heatmap_data_greek.columns = ['Î§Î±ÏÎ¬', 'Î›ÏÏ€Î·', 'Î˜Ï…Î¼ÏŒÏ‚', 'Î¦ÏŒÎ²Î¿Ï‚', 'ÎˆÎºÏ€Î»Î·Î¾Î·', 'Î‘Î·Î´Î¯Î±', 'Î•Î¼Ï€Î¹ÏƒÏ„Î¿ÏƒÏÎ½Î·', 'Î ÏÎ¿ÏƒÎ¼Î¿Î½Î®']

                        sns.heatmap(heatmap_data_greek, annot=True, cmap="YlGnBu",
                                   yticklabels=sentence_labels, fmt='.2f', ax=ax)
                        ax.set_title('ÎšÎ±Ï„Î±Î½Î¿Î¼Î® Î£Ï…Î½Î±Î¹ÏƒÎ¸Î·Î¼Î¬Ï„Ï‰Î½ ÏƒÎµ Î ÏÎ¿Ï„Î¬ÏƒÎµÎ¹Ï‚')
                        plt.tight_layout()
                        st.pyplot(fig)
                        plt.close(fig)
                    else:
                        st.info("Î”ÎµÎ½ ÎµÎ½Ï„Î¿Ï€Î¯ÏƒÏ„Î·ÎºÎ±Î½ ÏƒÎ·Î¼Î±Î½Ï„Î¹ÎºÎ¬ ÏƒÏ…Î½Î±Î¹ÏƒÎ¸Î®Î¼Î±Ï„Î± ÏƒÏ„Î¹Ï‚ ÎµÏ€Î¹Î¼Î­ÏÎ¿Ï…Ï‚ Ï€ÏÎ¿Ï„Î¬ÏƒÎµÎ¹Ï‚.")

                    with st.expander("Î ÏÎ¿Î²Î¿Î»Î® Î›ÎµÏ€Ï„Î¿Î¼ÎµÏÏÎ½ Î£Ï…Î½Î±Î¹ÏƒÎ¸Î·Î¼Î¬Ï„Ï‰Î½ Î ÏÎ¿Ï„Î¬ÏƒÎµÏ‰Î½", expanded=False):
                        cols = ['Sentence', 'Dominant Emotion'] + emotion_columns
                        st.dataframe(sentence_emotions[cols], width='stretch')

                except Exception as e:
                    st.error(f"Î£Ï†Î¬Î»Î¼Î± ÏƒÏ„Î·Î½ Î±Î½Î¬Î»Ï…ÏƒÎ· ÏƒÏ…Î½Î±Î¹ÏƒÎ¸Î·Î¼Î¬Ï„Ï‰Î½ ÏƒÎµ ÎµÏ€Î¯Ï€ÎµÎ´Î¿ Ï€ÏÏŒÏ„Î±ÏƒÎ·Ï‚: {str(e)}")

            elif analysis_tab == "Î˜ÎµÎ¼Î±Ï„Î¹ÎºÎ® ÎœÎ¿Î½Ï„ÎµÎ»Î¿Ï€Î¿Î¯Î·ÏƒÎ·":
                st.header('Î‘Î½Î¬Î»Ï…ÏƒÎ· Î˜ÎµÎ¼Î±Ï„Î¹ÎºÎ®Ï‚ ÎœÎ¿Î½Ï„ÎµÎ»Î¿Ï€Î¿Î¯Î·ÏƒÎ·Ï‚')

                st.markdown("""
                Î— Î¸ÎµÎ¼Î±Ï„Î¹ÎºÎ® Î¼Î¿Î½Ï„ÎµÎ»Î¿Ï€Î¿Î¯Î·ÏƒÎ· Î²Î¿Î·Î¸Î¬ ÏƒÏ„Î·Î½ Î±Î½Î±Î³Î½ÏÏÎ¹ÏƒÎ· Ï„Ï‰Î½ ÎºÏÏÎ¹Ï‰Î½ Î¸ÎµÎ¼Î¬Ï„Ï‰Î½ ÏƒÏ„Î¿ ÎºÎµÎ¯Î¼ÎµÎ½ÏŒ ÏƒÎ±Ï‚.
                Î‘Ï…Ï„Î® Î· Î±Î½Î¬Î»Ï…ÏƒÎ· Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯ **Latent Dirichlet Allocation (LDA)** Î® **Non-negative Matrix Factorization (NMF)**
                Î³Î¹Î± Ï„Î·Î½ Î±Î½Î±ÎºÎ¬Î»Ï…ÏˆÎ· ÎºÏÏ…Ï†ÏÎ½ Î¸ÎµÎ¼Î¬Ï„Ï‰Î½ Î²Î¬ÏƒÎµÎ¹ Ï€ÏÎ¿Ï„ÏÏ€Ï‰Î½ Î»Î­Î¾ÎµÏ‰Î½.
                """)

                if len(analyzer.sentences) < 3:
                    st.warning("Î— Î¸ÎµÎ¼Î±Ï„Î¹ÎºÎ® Î¼Î¿Î½Ï„ÎµÎ»Î¿Ï€Î¿Î¯Î·ÏƒÎ· Î±Ï€Î±Î¹Ï„ÎµÎ¯ Ï„Î¿Ï…Î»Î¬Ï‡Î¹ÏƒÏ„Î¿Î½ 3 Ï€ÏÎ¿Ï„Î¬ÏƒÎµÎ¹Ï‚. Î¤Î¿ ÎºÎµÎ¯Î¼ÎµÎ½ÏŒ ÏƒÎ±Ï‚ ÎµÎ¯Î½Î±Î¹ Ï€Î¿Î»Ï ÏƒÏÎ½Ï„Î¿Î¼Î¿ Î³Î¹Î± Î¿Ï…ÏƒÎ¹Î±ÏƒÏ„Î¹ÎºÎ® Î±Î½Î¬Î»Ï…ÏƒÎ· Î¸ÎµÎ¼Î¬Ï„Ï‰Î½.")
                else:
                    col1, col2, col3 = st.columns(3)

                    with col1:
                        method = st.selectbox(
                            "Î•Ï€Î¹Î»Î­Î¾Ï„Îµ ÎœÎ­Î¸Î¿Î´Î¿",
                            ["lda", "nmf"],
                            format_func=lambda x: "LDA (Latent Dirichlet Allocation)" if x == "lda" else "NMF (Non-negative Matrix Factorization)"
                        )

                    with col2:
                        n_topics = st.slider("Î‘ÏÎ¹Î¸Î¼ÏŒÏ‚ Î˜ÎµÎ¼Î¬Ï„Ï‰Î½", min_value=2, max_value=10, value=5)

                    with col3:
                        n_words = st.slider("Î›Î­Î¾ÎµÎ¹Ï‚ Î±Î½Î¬ Î˜Î­Î¼Î±", min_value=5, max_value=20, value=10)

                    try:
                        with st.spinner('Î•Î¾Î±Î³Ï‰Î³Î® Î¸ÎµÎ¼Î¬Ï„Ï‰Î½...'):
                            topic_results = analyzer.topic_modeling_lda(
                                n_topics=n_topics,
                                n_top_words=n_words,
                                method=method
                            )

                        if topic_results is None:
                            st.error("Î”ÎµÎ½ Î®Ï„Î±Î½ Î´Ï…Î½Î±Ï„Î® Î· ÎµÎºÏ„Î­Î»ÎµÏƒÎ· Î¸ÎµÎ¼Î±Ï„Î¹ÎºÎ®Ï‚ Î¼Î¿Î½Ï„ÎµÎ»Î¿Ï€Î¿Î¯Î·ÏƒÎ·Ï‚. Î’ÎµÎ²Î±Î¹Ï‰Î¸ÎµÎ¯Ï„Îµ ÏŒÏ„Î¹ Ï„Î¿ ÎºÎµÎ¯Î¼ÎµÎ½ÏŒ ÏƒÎ±Ï‚ Î­Ï‡ÎµÎ¹ Î±ÏÎºÎµÏ„ÏŒ Ï€ÎµÏÎ¹ÎµÏ‡ÏŒÎ¼ÎµÎ½Î¿ ÎºÎ±Î¹ Ï€Î¿Î¹ÎºÎ¹Î»Î¯Î±.")
                        else:
                            topics = topic_results['topics']
                            doc_topic_dist = topic_results['doc_topic_dist']
                            dominant_topics_df = topic_results['dominant_topics']

                            # Display topics
                            st.subheader("Î‘Î½Î±ÎºÎ±Î»Ï…Ï†Î¸Î­Î½Ï„Î± Î˜Î­Î¼Î±Ï„Î±")

                            cols_per_row = 2
                            topic_items = list(topics.items())

                            for i in range(0, len(topic_items), cols_per_row):
                                cols = st.columns(cols_per_row)
                                for j, (topic_name, topic_data) in enumerate(topic_items[i:i+cols_per_row]):
                                    with cols[j]:
                                        st.markdown(f"### {topic_name}")
                                        words_str = ", ".join(topic_data['words'][:8])
                                        st.info(f"**ÎšÎ¿ÏÏ…Ï†Î±Î¯ÎµÏ‚ Î»Î­Î¾ÎµÎ¹Ï‚:** {words_str}")

                            # Visualize topics
                            st.subheader("ÎšÎ±Ï„Î±Î½Î¿Î¼Î­Ï‚ Î›Î­Î¾ÎµÏ‰Î½ Î˜ÎµÎ¼Î¬Ï„Ï‰Î½")

                            for topic_name, topic_data in topics.items():
                                try:
                                    fig, ax = create_safe_plot(figsize=(10, 4))

                                    words = topic_data['words'][:n_words]
                                    weights = topic_data['weights'][:n_words]

                                    weights = np.array(weights)
                                    weights = weights / weights.sum()

                                    colors_palette = plt.cm.viridis(np.linspace(0.3, 0.9, len(words)))

                                    bars = ax.barh(range(len(words)), weights, color=colors_palette)
                                    ax.set_yticks(range(len(words)))
                                    ax.set_yticklabels(words)
                                    ax.set_xlabel('Î’Î¬ÏÎ¿Ï‚')
                                    ax.set_title(f'{topic_name} - ÎšÎ¿ÏÏ…Ï†Î±Î¯ÎµÏ‚ Î›Î­Î¾ÎµÎ¹Ï‚')
                                    ax.invert_yaxis()

                                    for i, (bar, weight) in enumerate(zip(bars, weights)):
                                        width = bar.get_width()
                                        ax.text(width, bar.get_y() + bar.get_height()/2,
                                               f' {weight:.3f}',
                                               va='center', ha='left', fontsize=9)

                                    plt.tight_layout()
                                    st.pyplot(fig)
                                    plt.close(fig)

                                except Exception as e:
                                    st.error(f"Î£Ï†Î¬Î»Î¼Î± Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î±Ï‚ Î¿Ï€Ï„Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ·Ï‚ Î³Î¹Î± {topic_name}: {str(e)}")

                            # Topic distribution
                            st.subheader("ÎšÎ±Ï„Î±Î½Î¿Î¼Î® Î˜ÎµÎ¼Î¬Ï„Ï‰Î½")

                            try:
                                fig, ax = create_safe_plot(figsize=(12, 6))

                                display_limit = min(30, len(doc_topic_dist))
                                heatmap_data = doc_topic_dist[:display_limit]

                                im = ax.imshow(heatmap_data.T, aspect='auto', cmap='YlOrRd')

                                ax.set_yticks(range(n_topics))
                                ax.set_yticklabels([f'Î˜Î­Î¼Î± {i+1}' for i in range(n_topics)])
                                ax.set_xlabel('Î”ÎµÎ¯ÎºÏ„Î·Ï‚ Î ÏÏŒÏ„Î±ÏƒÎ·Ï‚')
                                ax.set_ylabel('Î˜Î­Î¼Î±Ï„Î±')
                                ax.set_title('ÎšÎ±Ï„Î±Î½Î¿Î¼Î® Î˜ÎµÎ¼Î¬Ï„Ï‰Î½ ÏƒÎµ Î ÏÎ¿Ï„Î¬ÏƒÎµÎ¹Ï‚')

                                cbar = plt.colorbar(im, ax=ax)
                                cbar.set_label('Î’Î¬ÏÎ¿Ï‚ Î˜Î­Î¼Î±Ï„Î¿Ï‚')

                                plt.tight_layout()
                                st.pyplot(fig)
                                plt.close(fig)

                            except Exception as e:
                                st.error(f"Î£Ï†Î¬Î»Î¼Î± Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î±Ï‚ Î¿Ï€Ï„Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ·Ï‚ ÎºÎ±Ï„Î±Î½Î¿Î¼Î®Ï‚ Î¸ÎµÎ¼Î¬Ï„Ï‰Î½: {str(e)}")

                            # Overall topic proportions
                            st.subheader("Î£Ï…Î½Î¿Î»Î¹ÎºÎ­Ï‚ Î‘Î½Î±Î»Î¿Î³Î¯ÎµÏ‚ Î˜ÎµÎ¼Î¬Ï„Ï‰Î½")

                            try:
                                avg_topic_weights = doc_topic_dist.mean(axis=0)

                                fig, ax = plt.subplots(figsize=(8, 8))

                                colors_pie = plt.cm.Set3(range(n_topics))
                                patches, texts, autotexts = ax.pie(
                                    avg_topic_weights,
                                    labels=[f'Î˜Î­Î¼Î± {i+1}' for i in range(n_topics)],
                                    autopct='%1.1f%%',
                                    colors=colors_pie,
                                    startangle=90
                                )

                                for autotext in autotexts:
                                    autotext.set_color('white')
                                    autotext.set_fontweight('bold')

                                ax.axis('equal')
                                ax.set_title('ÎœÎ­ÏƒÎµÏ‚ Î‘Î½Î±Î»Î¿Î³Î¯ÎµÏ‚ Î˜ÎµÎ¼Î¬Ï„Ï‰Î½ ÏƒÏ„Î¿ ÎšÎµÎ¯Î¼ÎµÎ½Î¿')
                                st.pyplot(fig)
                                plt.close(fig)

                            except Exception as e:
                                st.error(f"Î£Ï†Î¬Î»Î¼Î± Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î±Ï‚ Î³ÏÎ±Ï†Î®Î¼Î±Ï„Î¿Ï‚ Î±Î½Î±Î»Î¿Î³Î¹ÏÎ½ Î¸ÎµÎ¼Î¬Ï„Ï‰Î½: {str(e)}")

                            # Sentence-topic assignments
                            st.subheader("Î‘Î½Î¬Î¸ÎµÏƒÎ· Î ÏÎ¿Ï„Î¬ÏƒÎµÏ‰Î½ ÏƒÎµ Î˜Î­Î¼Î±Ï„Î±")

                            st.markdown("ÎšÎ¬Î¸Îµ Ï€ÏÏŒÏ„Î±ÏƒÎ·Î±Î½Î±Ï„Î¯Î¸ÎµÏ„Î±Î¹ ÏƒÏ„Î¿ Ï€Î¹Î¿ ÎºÏ…ÏÎ¯Î±ÏÏ‡Î¿ Î¸Î­Î¼Î± Ï„Î·Ï‚:")

                            topic_counts = dominant_topics_df['Dominant Topic'].value_counts()
                            col1, col2 = st.columns([1, 2])

                            with col1:
                                st.markdown("**ÎšÎ±Ï„Î±Î½Î¿Î¼Î® Î˜ÎµÎ¼Î¬Ï„Ï‰Î½:**")
                                for topic, count in topic_counts.items():
                                    percentage = (count / len(dominant_topics_df)) * 100
                                    st.metric(topic, count, f"{percentage:.1f}%")

                            with col2:
                                try:
                                    fig, ax = create_safe_plot()
                                    colors_bar = plt.cm.Set3(range(len(topic_counts)))
                                    bars = ax.bar(topic_counts.index, topic_counts.values, color=colors_bar)
                                    ax.set_xlabel('Î˜Î­Î¼Î±')
                                    ax.set_ylabel('Î‘ÏÎ¹Î¸Î¼ÏŒÏ‚ Î ÏÎ¿Ï„Î¬ÏƒÎµÏ‰Î½')
                                    ax.set_title('Î ÏÎ¿Ï„Î¬ÏƒÎµÎ¹Ï‚ Î±Î½Î¬ Î˜Î­Î¼Î±')

                                    for bar in bars:
                                        height = bar.get_height()
                                        ax.text(bar.get_x() + bar.get_width()/2., height,
                                               f'{int(height)}',
                                               ha='center', va='bottom')

                                    plt.xticks(rotation=45)
                                    plt.tight_layout()
                                    st.pyplot(fig)
                                    plt.close(fig)

                                except Exception as e:
                                    st.error(f"Î£Ï†Î¬Î»Î¼Î± Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î±Ï‚ Î³ÏÎ±Ï†Î®Î¼Î±Ï„Î¿Ï‚ ÎºÎ±Ï„Î±Î½Î¿Î¼Î®Ï‚ Î¸ÎµÎ¼Î¬Ï„Ï‰Î½: {str(e)}")

                            with st.expander("Î ÏÎ¿Î²Î¿Î»Î® Î›ÎµÏ€Ï„Î¿Î¼ÎµÏÏÎ½ Î‘Î½Î±Î¸Î­ÏƒÎµÏ‰Î½ Î ÏÎ¿Ï„Î¬ÏƒÎµÏ‰Î½-Î˜ÎµÎ¼Î¬Ï„Ï‰Î½", expanded=False):
                                sorted_df = dominant_topics_df.sort_values('Topic Weight', ascending=False)
                                st.dataframe(sorted_df, width='stretch')

                    except Exception as e:
                        st.error(f"Î£Ï†Î¬Î»Î¼Î± ÎµÎºÏ„Î­Î»ÎµÏƒÎ·Ï‚ Î¸ÎµÎ¼Î±Ï„Î¹ÎºÎ®Ï‚ Î¼Î¿Î½Ï„ÎµÎ»Î¿Ï€Î¿Î¯Î·ÏƒÎ·Ï‚: {str(e)}")
                        st.info("Î”Î¿ÎºÎ¹Î¼Î¬ÏƒÏ„Îµ Î½Î± Ï€ÏÎ¿ÏƒÎ±ÏÎ¼ÏŒÏƒÎµÏ„Îµ Ï„Î¿Î½ Î±ÏÎ¹Î¸Î¼ÏŒ Î¸ÎµÎ¼Î¬Ï„Ï‰Î½ Î® Î²ÎµÎ²Î±Î¹Ï‰Î¸ÎµÎ¯Ï„Îµ ÏŒÏ„Î¹ Ï„Î¿ ÎºÎµÎ¯Î¼ÎµÎ½ÏŒ ÏƒÎ±Ï‚ Î­Ï‡ÎµÎ¹ ÎµÏ€Î±ÏÎºÎ­Ï‚ Ï€ÎµÏÎ¹ÎµÏ‡ÏŒÎ¼ÎµÎ½Î¿.")

        except ValueError as e:
            if "Text file too large" in str(e):
                st.error(f"Î¤Î¿ Î±ÏÏ‡ÎµÎ¯Î¿ ÎºÎµÎ¹Î¼Î­Î½Î¿Ï… ÎµÎ¯Î½Î±Î¹ Ï€Î¿Î»Ï Î¼ÎµÎ³Î¬Î»Î¿. Î¤Î¿ Î¼Î­Î³Î¹ÏƒÏ„Î¿ ÎµÏ€Î¹Ï„ÏÎµÏ€ÏŒÎ¼ÎµÎ½Î¿ Î¼Î­Î³ÎµÎ¸Î¿Ï‚ ÎµÎ¯Î½Î±Î¹ {MAX_TEXT_SIZE_MB}MB.")
                st.info("Î Î±ÏÎ±ÎºÎ±Î»Ï Ï‡Ï‰ÏÎ¯ÏƒÏ„Îµ Ï„Î¿ Î±ÏÏ‡ÎµÎ¯Î¿ ÏƒÎµ Î¼Î¹ÎºÏÏŒÏ„ÎµÏÎ± Ï„Î¼Î®Î¼Î±Ï„Î± Î® Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÏ„Îµ Î­Î½Î± Î¼Î¹ÎºÏÏŒÏ„ÎµÏÎ¿ Î±ÏÏ‡ÎµÎ¯Î¿.")
            else:
                st.error(f"Î£Ï†Î¬Î»Î¼Î± ÎµÏ€Î¹ÎºÏÏÏ‰ÏƒÎ·Ï‚ Î±ÏÏ‡ÎµÎ¯Î¿Ï…: {str(e)}")
        except Exception as e:
            st.error(f"Î Î±ÏÎ¿Ï…ÏƒÎ¹Î¬ÏƒÏ„Î·ÎºÎµ ÏƒÏ†Î¬Î»Î¼Î± ÎºÎ±Ï„Î¬ Ï„Î·Î½ ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± Ï„Î¿Ï… Î±ÏÏ‡ÎµÎ¯Î¿Ï…: {str(e)}")
            st.error("Î Î±ÏÎ±ÎºÎ±Î»Ï Î²ÎµÎ²Î±Î¹Ï‰Î¸ÎµÎ¯Ï„Îµ ÏŒÏ„Î¹ Ï„Î¿ Î±ÏÏ‡ÎµÎ¯Î¿ ÎµÎ¯Î½Î±Î¹ Î­Î³ÎºÏ…ÏÎ¿ Î±ÏÏ‡ÎµÎ¯Î¿ ÎºÎµÎ¹Î¼Î­Î½Î¿Ï… ÎºÎ±Î¹ Î´Î¿ÎºÎ¹Î¼Î¬ÏƒÏ„Îµ Î¾Î±Î½Î¬.")

            # Cleanup on error
            gc.collect()

if __name__ == "__main__":
    main()
